{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 9.2 원하는 숫자 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ..\\data\\mnist\\data\\train-images-idx3-ubyte.gz\n",
      "Extracting ..\\data\\mnist\\data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ..\\data\\mnist\\data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ..\\data\\mnist\\data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"..\\data\\mnist\\data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# 옵션 설정\n",
    "###########\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10\n",
    "\n",
    "\n",
    "##############\n",
    "# 신경망 모델 구성\n",
    "#############\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "# 노이즈에 레이블 데이터를 힌트로 넣어줌. \n",
    "Y = tf.placeholder(tf.float32, [None, n_class])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "\n",
    "# 생성자 신경망 구성\n",
    "# 학습 시 각 신경망의 변수들을 따로 학습시킨다.\n",
    "def generator(noise, labels):\n",
    "    # tf.layers 사용 -> scope 지정 : scope에 해당하는 변수들만 불러올 수 있다.\n",
    "    with tf.variable_scope('generator'):\n",
    "        #concat 함수 : noise값에 labels 정보를 추가\n",
    "        inputs = tf.concat([noise, labels], 1)\n",
    "        # 은닉층 생성\n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                activation = tf.nn.relu)\n",
    "        # 출력층 구성\n",
    "        output = tf.layers.dense(hidden, n_input,\n",
    "                                activation = tf.nn.sigmoid)\n",
    "    return output\n",
    "\n",
    "\n",
    "        \n",
    "# 구분자 신경망 구성\n",
    "def discriminator(inputs, labels, reuse=None):\n",
    "    # 진짜 이미지를 판별할 때와 가짜 이미지를 판별할때 똑같은 변수 사용해야함.\n",
    "    # scope.reuse_variables 함수로 이전에 사용한 변수 재사용\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden,\n",
    "                                activation = tf.nn.relu)\n",
    "        # 손실값에 sigmoid_entropy 함수 사용하기 위해 활성화함수 안씀\n",
    "        output = tf.layers.dense(hidden, 1,\n",
    "                                activation = None)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "# 무작위 노이즈 생성 함수\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(-1., 1., size=[batch_size, n_noise])\n",
    "\n",
    "\n",
    "\n",
    "# 레이블 정보 추가 -> 레이블에 해당하는 이미지 생성 유도\n",
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X, Y)\n",
    "# 가짜 이미지 구분자 -> 진짜 이미지 구분에 사용한 변수 사용하도록 reuse=True\n",
    "D_gene = discriminator(G, Y, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###########\n",
    "# 손실함수\n",
    "###########\n",
    "# D_real : 1에 가깝게, 실제 이미지는 진짜라고 판별\n",
    "# D_gene : 0에 가깝게, 가짜 이미지는 가짜라고 판별\n",
    "# sigmoid_cross_entropy_with_logits 함수 사용\n",
    "\n",
    "loss_D_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            # ones_like : D_real 결과값과 D_real 크기만큼 1로 채운 값들을 비교\n",
    "            logits=D_real, labels=tf.ones_like(D_real)))\n",
    "\n",
    "loss_D_gene = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=D_gene, labels=tf.ones_like(D_gene)))\n",
    "\n",
    "# 이 값을 최소화하면 구분자(경찰)을 학습시킬 수 있다.\n",
    "loss_D = loss_D_real + loss_D_gene\n",
    "\n",
    "\n",
    "\n",
    "# loss_G ㅣ 생성자 학습시키기 위한 손실값\n",
    "loss_G = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:: 0000 D loss:: 0.0002446 G loss:: 8.326\n",
      "Epoch:: 0001 D loss:: 1.074e-05 G loss:: 11.45\n",
      "Epoch:: 0002 D loss:: 3.744e-06 G loss:: 12.5\n",
      "Epoch:: 0003 D loss:: 1.924e-06 G loss:: 13.17\n",
      "Epoch:: 0004 D loss:: 1.167e-06 G loss:: 13.67\n",
      "Epoch:: 0005 D loss:: 7.655e-07 G loss:: 14.09\n",
      "Epoch:: 0006 D loss:: 5.122e-07 G loss:: 14.49\n",
      "Epoch:: 0007 D loss:: 3.594e-07 G loss:: 14.84\n",
      "Epoch:: 0008 D loss:: 2.52e-07 G loss:: 15.2\n",
      "Epoch:: 0009 D loss:: 1.852e-07 G loss:: 15.51\n",
      "Epoch:: 0010 D loss:: 1.34e-07 G loss:: 15.83\n",
      "Epoch:: 0011 D loss:: 1.006e-07 G loss:: 16.12\n",
      "Epoch:: 0012 D loss:: 1.853e-06 G loss:: 13.21\n",
      "Epoch:: 0013 D loss:: 7.325e-07 G loss:: 14.13\n",
      "Epoch:: 0014 D loss:: 3.997e-07 G loss:: 14.74\n",
      "Epoch:: 0015 D loss:: 2.54e-07 G loss:: 15.19\n",
      "Epoch:: 0016 D loss:: 1.611e-07 G loss:: 15.65\n",
      "Epoch:: 0017 D loss:: 1.118e-07 G loss:: 16.01\n",
      "Epoch:: 0018 D loss:: 7.734e-08 G loss:: 16.38\n",
      "Epoch:: 0019 D loss:: 5.46e-08 G loss:: 16.73\n",
      "Epoch:: 0020 D loss:: 3.983e-08 G loss:: 17.04\n",
      "Epoch:: 0021 D loss:: 2.923e-08 G loss:: 17.35\n",
      "Epoch:: 0022 D loss:: 2.148e-08 G loss:: 17.66\n",
      "Epoch:: 0023 D loss:: 1.59e-08 G loss:: 17.96\n",
      "Epoch:: 0024 D loss:: 1.174e-08 G loss:: 18.27\n",
      "Epoch:: 0025 D loss:: 8.801e-09 G loss:: 18.55\n",
      "Epoch:: 0026 D loss:: 6.528e-09 G loss:: 18.85\n",
      "Epoch:: 0027 D loss:: 4.92e-09 G loss:: 19.13\n",
      "Epoch:: 0028 D loss:: 3.709e-09 G loss:: 19.42\n",
      "Epoch:: 0029 D loss:: 2.771e-09 G loss:: 19.71\n",
      "Epoch:: 0030 D loss:: 2.156e-09 G loss:: 19.96\n",
      "Epoch:: 0031 D loss:: 1.632e-09 G loss:: 20.24\n",
      "Epoch:: 0032 D loss:: 1.256e-09 G loss:: 20.5\n",
      "Epoch:: 0033 D loss:: 9.546e-10 G loss:: 20.77\n",
      "Epoch:: 0034 D loss:: 7.484e-10 G loss:: 21.02\n",
      "Epoch:: 0035 D loss:: 5.819e-10 G loss:: 21.27\n",
      "Epoch:: 0036 D loss:: 4.496e-10 G loss:: 21.53\n",
      "Epoch:: 0037 D loss:: 3.544e-10 G loss:: 21.76\n",
      "Epoch:: 0038 D loss:: 2.813e-10 G loss:: 21.99\n",
      "Epoch:: 0039 D loss:: 2.252e-10 G loss:: 22.22\n",
      "Epoch:: 0040 D loss:: 1.783e-10 G loss:: 22.45\n",
      "Epoch:: 0041 D loss:: 1.46e-10 G loss:: 22.65\n",
      "Epoch:: 0042 D loss:: 1.19e-10 G loss:: 22.85\n",
      "Epoch:: 0043 D loss:: 9.812e-11 G loss:: 23.05\n",
      "Epoch:: 0044 D loss:: 8.444e-11 G loss:: 23.2\n",
      "Epoch:: 0045 D loss:: 7.095e-11 G loss:: 23.37\n",
      "Epoch:: 0046 D loss:: 6.042e-11 G loss:: 23.53\n",
      "Epoch:: 0047 D loss:: 5.206e-11 G loss:: 23.68\n",
      "Epoch:: 0048 D loss:: 4.537e-11 G loss:: 23.82\n",
      "Epoch:: 0049 D loss:: 4.023e-11 G loss:: 23.94\n",
      "Epoch:: 0050 D loss:: 3.604e-11 G loss:: 24.05\n",
      "Epoch:: 0051 D loss:: 3.219e-11 G loss:: 24.16\n",
      "Epoch:: 0052 D loss:: 2.866e-11 G loss:: 24.28\n",
      "Epoch:: 0053 D loss:: 2.627e-11 G loss:: 24.36\n",
      "Epoch:: 0054 D loss:: 2.44e-11 G loss:: 24.44\n",
      "Epoch:: 0055 D loss:: 2.228e-11 G loss:: 24.53\n",
      "Epoch:: 0056 D loss:: 2.077e-11 G loss:: 24.6\n",
      "Epoch:: 0057 D loss:: 1.929e-11 G loss:: 24.67\n",
      "Epoch:: 0058 D loss:: 1.792e-11 G loss:: 24.75\n",
      "Epoch:: 0059 D loss:: 1.681e-11 G loss:: 24.81\n",
      "Epoch:: 0060 D loss:: 1.594e-11 G loss:: 24.86\n",
      "Epoch:: 0061 D loss:: 1.503e-11 G loss:: 24.92\n",
      "Epoch:: 0062 D loss:: 1.444e-11 G loss:: 24.96\n",
      "Epoch:: 0063 D loss:: 1.333e-11 G loss:: 25.04\n",
      "Epoch:: 0064 D loss:: 1.29e-11 G loss:: 25.08\n",
      "Epoch:: 0065 D loss:: 1.222e-11 G loss:: 25.13\n",
      "Epoch:: 0066 D loss:: 1.169e-11 G loss:: 25.18\n",
      "Epoch:: 0067 D loss:: 1.118e-11 G loss:: 25.22\n",
      "Epoch:: 0068 D loss:: 1.078e-11 G loss:: 25.25\n",
      "Epoch:: 0069 D loss:: 1.044e-11 G loss:: 25.29\n",
      "Epoch:: 0070 D loss:: 9.977e-12 G loss:: 25.33\n",
      "Epoch:: 0071 D loss:: 9.62e-12 G loss:: 25.37\n",
      "Epoch:: 0072 D loss:: 9.289e-12 G loss:: 25.4\n",
      "Epoch:: 0073 D loss:: 9.066e-12 G loss:: 25.43\n",
      "Epoch:: 0074 D loss:: 8.744e-12 G loss:: 25.47\n",
      "Epoch:: 0075 D loss:: 8.272e-12 G loss:: 25.52\n",
      "Epoch:: 0076 D loss:: 8.197e-12 G loss:: 25.53\n",
      "Epoch:: 0077 D loss:: 7.947e-12 G loss:: 25.56\n",
      "Epoch:: 0078 D loss:: 7.67e-12 G loss:: 25.6\n",
      "Epoch:: 0079 D loss:: 7.477e-12 G loss:: 25.62\n",
      "Epoch:: 0080 D loss:: 7.264e-12 G loss:: 25.65\n",
      "Epoch:: 0081 D loss:: 6.947e-12 G loss:: 25.69\n",
      "Epoch:: 0082 D loss:: 6.906e-12 G loss:: 25.7\n",
      "Epoch:: 0083 D loss:: 6.715e-12 G loss:: 25.73\n",
      "Epoch:: 0084 D loss:: 6.508e-12 G loss:: 25.76\n",
      "Epoch:: 0085 D loss:: 6.405e-12 G loss:: 25.78\n",
      "Epoch:: 0086 D loss:: 6.188e-12 G loss:: 25.81\n",
      "Epoch:: 0087 D loss:: 6.087e-12 G loss:: 25.83\n",
      "Epoch:: 0088 D loss:: 5.993e-12 G loss:: 25.84\n",
      "Epoch:: 0089 D loss:: 5.772e-12 G loss:: 25.88\n",
      "Epoch:: 0090 D loss:: 5.699e-12 G loss:: 25.89\n",
      "Epoch:: 0091 D loss:: 5.584e-12 G loss:: 25.91\n",
      "Epoch:: 0092 D loss:: 5.537e-12 G loss:: 25.92\n",
      "Epoch:: 0093 D loss:: 5.359e-12 G loss:: 25.95\n",
      "Epoch:: 0094 D loss:: 5.204e-12 G loss:: 25.98\n",
      "Epoch:: 0095 D loss:: 5.074e-12 G loss:: 26.01\n",
      "Epoch:: 0096 D loss:: 5.033e-12 G loss:: 26.02\n",
      "Epoch:: 0097 D loss:: 4.972e-12 G loss:: 26.03\n",
      "Epoch:: 0098 D loss:: 4.828e-12 G loss:: 26.06\n",
      "Epoch:: 0099 D loss:: 4.736e-12 G loss:: 26.08\n",
      "최적화완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# discriminator, generator 스코프에서 사용된 변수들을 가져온 뒤\n",
    "# 이 변수들을 최적화할 각 손실함수와 함께 최적화 함수에 넣어\n",
    "# 학습 모델 구성 완료\n",
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                          scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                          scope='generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D,\n",
    "                                           var_list = vars_D)\n",
    "\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G,\n",
    "                                           var_list = vars_G)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "# 학습 진행\n",
    "###########\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                feed_dict={X: batch_xs, Y:batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                feed_dict={Y: batch_ys, Z: noise})\n",
    "    print('Epoch::','%04d' % epoch,\n",
    "         'D loss:: {:.4}'.format(loss_val_D),\n",
    "         'G loss:: {:.4}'.format(loss_val_G))\n",
    "    \n",
    "\n",
    "    #############\n",
    "    # 결과 확인 (for문 안에서)\n",
    "    # 0, 9, 19, 29..마다 생성기로 이미지를 생성하여 확인\n",
    "    ############\n",
    "\n",
    "    # 노이즈를 만들고 생성자 G에 넣어 결과값을 만든다.\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Y:mnist.test.labels[:sample_size],\n",
    "                                         Z: noise})\n",
    "\n",
    "\n",
    "        # 노이즈 결과값을 28*28 크기의 가짜 이미지로 만들고\n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "\n",
    "        # 위 : 진짜 이미지, 아래 : 생성한 이미지\n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "\n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28,28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28,28)))\n",
    "        # samples 폴더에 저장\n",
    "        plt.savefig('samples2\\{}.png'.format(str(epoch).zfill(3),\n",
    "                                           bbox_inches='tight'))\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "print('최적화완료!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
