{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 9. GAN\n",
    "- 비지도 학습\n",
    "- 결과물을 생성하는 모델\n",
    "- 서로 대립하는 두 신경망을 경쟁시켜가며 결과물 생성 방법을 학습한다.\n",
    "    - 위조지폐범(생성자)은 경찰(구분자)를 최대한 속이려고 노력하고, 경찰은 위조한 지폐를 최대한 감별하려고 노력한다.\n",
    "    \n",
    "    ![GAN](./img/img1.JPG)\n",
    "    \n",
    "1. 구분자에게 실제 이미지를 주고, 그 이미지가 진짜임을 판단하게 한다.\n",
    "2. 생성자를 통해 노이즈로부터 임의의 이미지를 만들고 \n",
    "3. 임의의 이미지를 같은 구분자를 통해 진짜인지 판단하게 한다.\n",
    "\n",
    "\n",
    "#### 9.1 GAN 기본 모델 구현\n",
    "- MNIST 손글씨를 무작위로 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ..\\data\\mnist\\data\\train-images-idx3-ubyte.gz\n",
      "Extracting ..\\data\\mnist\\data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ..\\data\\mnist\\data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ..\\data\\mnist\\data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"..\\data\\mnist\\data\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############\n",
    "# 하이퍼파라미터\n",
    "###############\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128      # 생성자의 입력값으로 사용할 노이즈 크기\n",
    "                    # 랜덤한 노이즈를 입력하고, 그 노이즈에서 손글씨 이미지를 \n",
    "                    # 무작위로 생성\n",
    "\n",
    "\n",
    "############\n",
    "# 모델 구성\n",
    "###########\n",
    "# 실제 이미지\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "# 가짜 이미지 (노이즈에서 생성)\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
    "\n",
    "\n",
    "# 생성자\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "\n",
    "# 구분자\n",
    "# 진짜와 얼마나 가까운가를 판단하는 값으로 0~1 사이의 값을 출력한다. \n",
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# 실제 이미지를 판별하는 구분자 신경망과\n",
    "# 생성한 이미지를 판별하는 구분자 신경망을\n",
    "# 같은 변수를 사용해야 한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# 생성자, 구분자 신경망 구성\n",
    "#################\n",
    "\n",
    "# 생성자 신경망\n",
    "def generator(noise_z):\n",
    "    # 노이즈를 받아 은닉층 생성\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)\n",
    "    # 은닉층에서 실제 이미지와 같은 크기의 결과값 출력\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# 구분자 신경망\n",
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# 무작위 노이즈 생성 함수\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))\n",
    "\n",
    "# 노이즈 Z를 이용해 가짜 이미지를 만들 생성자 G를 만들고\n",
    "G = generator(Z)\n",
    "# G가 만든 가짜 이미지와 진짜 이미지 X를 각각 구분자에 넣어 \n",
    "# 입력한 이미지가 진짜 이미지인지 판별하게 한다.\n",
    "D_gene = discriminator(G)\n",
    "D_real = discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############\n",
    "# 손실 함수 \n",
    "############\n",
    "# 생성자가 만든 이미지를 구분자가 가짜라고 판단하도록 하는 손실값(경찰 학습용)\n",
    "# 진짜 이미지 판별값(D_real) : 1에 가까워야 함.\n",
    "# 가짜 이미지 판별값(D_gene) : 0에 가까워야 함.\n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))\n",
    "\n",
    "# 진짜라고 판단하도록 만드는 손실값(위조지폐범 학습용)\n",
    "# 가짜 이미지 판별값(D_gene) : 1에 가깜게 만든다.\n",
    "# 즉 가짜 이미지도 진자같다고 판별해야 한다.\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GAN 학습은 loss_D와 loss_G 모두를 최대화 하는 것이다.  \n",
    "다만 서로 연관되어 있어서 두 손실값이 항상 같이 증가하지 않는다.  \n",
    "loss_D ↑ -> loss_G ↓\n",
    "loss_D ↓ -> loss_G ↑\n",
    "\n",
    "![손실 그래프](./img/img2.JPG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:: 0000 D loss:: -0.5391 G loss:: -2.059\n",
      "Epoch:: 0001 D loss:: -0.1612 G loss:: -2.912\n",
      "Epoch:: 0002 D loss:: -0.1536 G loss:: -2.869\n",
      "Epoch:: 0003 D loss:: -0.4849 G loss:: -1.636\n",
      "Epoch:: 0004 D loss:: -0.494 G loss:: -1.72\n",
      "Epoch:: 0005 D loss:: -0.2243 G loss:: -2.464\n",
      "Epoch:: 0006 D loss:: -0.2135 G loss:: -2.969\n",
      "Epoch:: 0007 D loss:: -0.3841 G loss:: -2.298\n",
      "Epoch:: 0008 D loss:: -0.3373 G loss:: -2.455\n",
      "Epoch:: 0009 D loss:: -0.3452 G loss:: -2.339\n",
      "Epoch:: 0010 D loss:: -0.4167 G loss:: -2.406\n",
      "Epoch:: 0011 D loss:: -0.3426 G loss:: -2.361\n",
      "Epoch:: 0012 D loss:: -0.2847 G loss:: -2.667\n",
      "Epoch:: 0013 D loss:: -0.5783 G loss:: -2.081\n",
      "Epoch:: 0014 D loss:: -0.292 G loss:: -2.68\n",
      "Epoch:: 0015 D loss:: -0.3525 G loss:: -2.247\n",
      "Epoch:: 0016 D loss:: -0.3106 G loss:: -2.606\n",
      "Epoch:: 0017 D loss:: -0.438 G loss:: -2.188\n",
      "Epoch:: 0018 D loss:: -0.3761 G loss:: -2.65\n",
      "Epoch:: 0019 D loss:: -0.3103 G loss:: -2.817\n",
      "Epoch:: 0020 D loss:: -0.3559 G loss:: -2.7\n",
      "Epoch:: 0021 D loss:: -0.3637 G loss:: -2.7\n",
      "Epoch:: 0022 D loss:: -0.4608 G loss:: -2.769\n",
      "Epoch:: 0023 D loss:: -0.3335 G loss:: -2.995\n",
      "Epoch:: 0024 D loss:: -0.3908 G loss:: -2.804\n",
      "Epoch:: 0025 D loss:: -0.3725 G loss:: -2.454\n",
      "Epoch:: 0026 D loss:: -0.4848 G loss:: -2.475\n",
      "Epoch:: 0027 D loss:: -0.3503 G loss:: -2.791\n",
      "Epoch:: 0028 D loss:: -0.4608 G loss:: -2.861\n",
      "Epoch:: 0029 D loss:: -0.3177 G loss:: -2.994\n",
      "Epoch:: 0030 D loss:: -0.4088 G loss:: -2.593\n",
      "Epoch:: 0031 D loss:: -0.3201 G loss:: -2.817\n",
      "Epoch:: 0032 D loss:: -0.318 G loss:: -2.832\n",
      "Epoch:: 0033 D loss:: -0.5907 G loss:: -2.693\n",
      "Epoch:: 0034 D loss:: -0.4972 G loss:: -2.648\n",
      "Epoch:: 0035 D loss:: -0.5285 G loss:: -2.717\n",
      "Epoch:: 0036 D loss:: -0.5222 G loss:: -2.286\n",
      "Epoch:: 0037 D loss:: -0.3667 G loss:: -2.639\n",
      "Epoch:: 0038 D loss:: -0.4556 G loss:: -2.567\n",
      "Epoch:: 0039 D loss:: -0.476 G loss:: -2.575\n",
      "Epoch:: 0040 D loss:: -0.5577 G loss:: -2.657\n",
      "Epoch:: 0041 D loss:: -0.5734 G loss:: -2.181\n",
      "Epoch:: 0042 D loss:: -0.7819 G loss:: -2.093\n",
      "Epoch:: 0043 D loss:: -0.6446 G loss:: -2.286\n",
      "Epoch:: 0044 D loss:: -0.5745 G loss:: -3.003\n",
      "Epoch:: 0045 D loss:: -0.5304 G loss:: -2.539\n",
      "Epoch:: 0046 D loss:: -0.6512 G loss:: -2.25\n",
      "Epoch:: 0047 D loss:: -0.4593 G loss:: -2.191\n",
      "Epoch:: 0048 D loss:: -0.5763 G loss:: -2.626\n",
      "Epoch:: 0049 D loss:: -0.58 G loss:: -2.024\n",
      "Epoch:: 0050 D loss:: -0.5151 G loss:: -2.333\n",
      "Epoch:: 0051 D loss:: -0.7268 G loss:: -2.252\n",
      "Epoch:: 0052 D loss:: -0.5794 G loss:: -2.217\n",
      "Epoch:: 0053 D loss:: -0.6132 G loss:: -2.053\n",
      "Epoch:: 0054 D loss:: -0.6971 G loss:: -2.036\n",
      "Epoch:: 0055 D loss:: -0.5809 G loss:: -2.376\n",
      "Epoch:: 0056 D loss:: -0.6068 G loss:: -1.907\n",
      "Epoch:: 0057 D loss:: -0.5573 G loss:: -2.247\n",
      "Epoch:: 0058 D loss:: -0.7266 G loss:: -1.946\n",
      "Epoch:: 0059 D loss:: -0.6302 G loss:: -2.156\n",
      "Epoch:: 0060 D loss:: -0.5295 G loss:: -2.164\n",
      "Epoch:: 0061 D loss:: -0.8233 G loss:: -1.947\n",
      "Epoch:: 0062 D loss:: -0.5734 G loss:: -2.041\n",
      "Epoch:: 0063 D loss:: -0.6528 G loss:: -2.096\n",
      "Epoch:: 0064 D loss:: -0.6108 G loss:: -2.096\n",
      "Epoch:: 0065 D loss:: -0.6782 G loss:: -2.178\n",
      "Epoch:: 0066 D loss:: -0.6534 G loss:: -2.099\n",
      "Epoch:: 0067 D loss:: -0.8127 G loss:: -2.009\n",
      "Epoch:: 0068 D loss:: -0.715 G loss:: -2.155\n",
      "Epoch:: 0069 D loss:: -0.7451 G loss:: -1.984\n",
      "Epoch:: 0070 D loss:: -0.8579 G loss:: -2.114\n",
      "Epoch:: 0071 D loss:: -0.698 G loss:: -1.938\n",
      "Epoch:: 0072 D loss:: -0.7474 G loss:: -2.216\n",
      "Epoch:: 0073 D loss:: -0.7347 G loss:: -1.886\n",
      "Epoch:: 0074 D loss:: -0.7836 G loss:: -1.736\n",
      "Epoch:: 0075 D loss:: -0.5644 G loss:: -2.247\n",
      "Epoch:: 0076 D loss:: -0.8334 G loss:: -2.079\n",
      "Epoch:: 0077 D loss:: -0.6672 G loss:: -2.06\n",
      "Epoch:: 0078 D loss:: -0.6985 G loss:: -2.161\n",
      "Epoch:: 0079 D loss:: -0.6897 G loss:: -2.082\n",
      "Epoch:: 0080 D loss:: -0.6808 G loss:: -1.877\n",
      "Epoch:: 0081 D loss:: -0.7156 G loss:: -2.084\n",
      "Epoch:: 0082 D loss:: -0.7126 G loss:: -1.96\n",
      "Epoch:: 0083 D loss:: -0.7449 G loss:: -2.013\n",
      "Epoch:: 0084 D loss:: -0.6688 G loss:: -1.891\n",
      "Epoch:: 0085 D loss:: -0.6837 G loss:: -2.277\n",
      "Epoch:: 0086 D loss:: -0.6199 G loss:: -1.961\n",
      "Epoch:: 0087 D loss:: -0.636 G loss:: -1.912\n",
      "Epoch:: 0088 D loss:: -0.6333 G loss:: -2.185\n",
      "Epoch:: 0089 D loss:: -0.6408 G loss:: -1.995\n",
      "Epoch:: 0090 D loss:: -0.5743 G loss:: -1.97\n",
      "Epoch:: 0091 D loss:: -0.7375 G loss:: -2.026\n",
      "Epoch:: 0092 D loss:: -0.6641 G loss:: -1.988\n",
      "Epoch:: 0093 D loss:: -0.7384 G loss:: -1.753\n",
      "Epoch:: 0094 D loss:: -0.6781 G loss:: -2.113\n",
      "Epoch:: 0095 D loss:: -0.7541 G loss:: -1.989\n",
      "Epoch:: 0096 D loss:: -0.6397 G loss:: -2.124\n",
      "Epoch:: 0097 D loss:: -0.6731 G loss:: -2.013\n",
      "Epoch:: 0098 D loss:: -0.663 G loss:: -2.071\n",
      "Epoch:: 0099 D loss:: -0.649 G loss:: -2.239\n",
      "최적화완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 서로(생성자, 구분자) 학습할 때 변하지 않아야 하기 때문에\n",
    "#loss_D를 구할 때는 구분자 신경망에 사용되는 변수들만 사용\n",
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "# loss_G를 구할 때는 생성자 신경망에 사용되는 변수들만 사용\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]\n",
    "\n",
    "\n",
    "# 최적화 함수 구성\n",
    "# 최적화에 쓸 수 있는 함수는 minimize로 음수를 붙여 최대화 한다.\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list = D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list = G_var_list)\n",
    "\n",
    "    \n",
    "    \n",
    "#################\n",
    "# 신경망 모델 학습\n",
    "#################\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "# 손실값 2개 학습\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                feed_dict={X: batch_xs, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                feed_dict={Z: noise})\n",
    "    print('Epoch::','%04d' % epoch,\n",
    "         'D loss:: {:.4}'.format(loss_val_D),\n",
    "         'G loss:: {:.4}'.format(loss_val_G))\n",
    "    \n",
    "\n",
    "    ############\n",
    "    # 결과 확인 (for문 안에서)\n",
    "    # 0, 9, 19, 29..마다 생성기로 이미지를 생성하여 확인\n",
    "    ############\n",
    "\n",
    "    # 노이즈를 만들고 생성자 G에 넣어 결과값을 만든다.\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z: noise})\n",
    "\n",
    "\n",
    "    # 노이즈 결과값을 28*28 크기의 가짜 이미지로 만들고\n",
    "    fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        ax[i].set_axis_off()\n",
    "        ax[i].imshow(np.reshape(samples[i], (28,28)))\n",
    "    # samples 폴더에 저장\n",
    "    plt.savefig('samples\\{}.png'.format(str(epoch).zfill(3),\n",
    "                                       bbox_inches='tight'))\n",
    "\n",
    "    plt.close(fig)\n",
    "    \n",
    "print('최적화완료!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
