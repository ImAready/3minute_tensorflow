{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 10.3 Sequence to Sequence (Seq2Seq)\n",
    "\n",
    "- 구글 기계번역에 사용하는 신경망 모델\n",
    "- 순차적 정보를 입력받는 신경망(RNN)과 출력하는 신경망 조합.\n",
    "\n",
    "![개념도](.\\img\\img4.JPG)\n",
    "\n",
    "- 입력받기 위한 신경망인 인코더와 출력을 위한 신경망인 디코더로 구성\n",
    "    - 예: 인코더는 원문, 디코더는 인코더가 번역한 결과물을 입력받음. 그 후 디코더가 출력한 결과물을 번역된 결과물과 비교하면서 학습\n",
    "    \n",
    "---------  \n",
    "\n",
    "** Seq2Seq 모델을 이용해 번역 프로그램을 만들어본다.**\n",
    "- 네 글다의 영어 단어 입력\n",
    "- 두 글자의 한글 단어로 번역\n",
    "\n",
    "![번역모델](.\\img\\img5.JPG)\n",
    "\n",
    "- 특수한 심볼이 필요하다\n",
    "    - 디코더에 입력이 시작됨을 알려주는 심볼, 출력이 끝났음을 알려주는 심볼, 빈 데이터를 채울 때 사용하는 의미없는 빈 심볼 (위 그림에서 'S', 'E', 'P')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 글자를 학습시키기위해 원-핫 인코딩 형식으로 변경해야 한다.\n",
    "# 영어 알파벳과 한글들을 나열 -> 한 글자씩 배열에 삽입\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀키스사랑']\n",
    "\n",
    "# 배열에 넣은 글자들을 연관배열 형태로 변경\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)\n",
    "\n",
    "# 학습에 사용할 영단어와 한글단어의 쌍을 가진 데이터 정의\n",
    "seq_data = [\n",
    "    ['word','단어'],['wood','나무'],\n",
    "    ['game','놀이'],['girl','소녀'],\n",
    "    ['kiss','키스'],['love','사랑']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------\n",
    "입력, 출력 단어를 하나씩 배열로 만든 후 원-핫 인코딩 형식으로 만들어 주는 함수 생성\n",
    "\n",
    "1. 인코더 셀의 입력값을 위해 입력 단어를 한 글자씩 떼어 배열로 만든다.\n",
    "    - input = [num_dic[n] for n in seq[0]]\n",
    "2. 디코더 셀의 입력값을 위해 출력 단어의 글자들을 배열로 만들고, 시작을 나타내는 심볼 'S'를 맨 앞에 붙인다.\n",
    "    - output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "3. 학습을 위해 비교할 디코더 셀의 출력값을 만들고, 출력의 끝을 알려주는 심볼 'E'를 마지막에 붙인다.\n",
    "    - target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "4. 원-핫 인코딩한다.\n",
    "    - sparse_softmax 함수 사용을 위해 인덱스 숫자 그대로 사용\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        \n",
    "        input_batch.append(np.eye(dic_len)[input])\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target)\n",
    "    \n",
    "    return input_batch, output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############\n",
    "# 신경망 변수\n",
    "############\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "n_class = n_input = dic_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "############\n",
    "# 인코더 : [batch size, time steps, input size]\n",
    "enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "# 디코더\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "# 디코더 출력 : [batch size, time steps] 원-핫 인코딩 아니라서 차원이 하나 낮음\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "\n",
    "# 인코더 셀\n",
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell,\n",
    "                                            output_keep_prob = 0.5)\n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
    "                                           dtype = tf.float32)\n",
    "# 디코더 셀\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_dell = tf.nn.rnn_cell.DropoutWrapper(dec_cell,\n",
    "                                            output_keep_prob = 0.5)\n",
    "    # 초기 상태 값으로 인코더의 최종 상태값 입력(인코더->디코더 전파)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                           initial_state = enc_states, \n",
    "                                            dtype = tf.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------\n",
    "고수준 API에서는 가중치와 편향을 알아서 해준다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층 생성\n",
    "model = tf.layers.dense(outputs, n_class, activation = None)\n",
    "\n",
    "# 손실함수\n",
    "cost = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = model, labels = targets))\n",
    "\n",
    "# 최적화\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:: 0001 cost = 3.787253\n",
      "Epoch:: 0002 cost = 2.386073\n",
      "Epoch:: 0003 cost = 1.199667\n",
      "Epoch:: 0004 cost = 0.612130\n",
      "Epoch:: 0005 cost = 0.276811\n",
      "Epoch:: 0006 cost = 0.146814\n",
      "Epoch:: 0007 cost = 0.073030\n",
      "Epoch:: 0008 cost = 0.037879\n",
      "Epoch:: 0009 cost = 0.021718\n",
      "Epoch:: 0010 cost = 0.013530\n",
      "Epoch:: 0011 cost = 0.009881\n",
      "Epoch:: 0012 cost = 0.006638\n",
      "Epoch:: 0013 cost = 0.003945\n",
      "Epoch:: 0014 cost = 0.002449\n",
      "Epoch:: 0015 cost = 0.001665\n",
      "Epoch:: 0016 cost = 0.001223\n",
      "Epoch:: 0017 cost = 0.000943\n",
      "Epoch:: 0018 cost = 0.000750\n",
      "Epoch:: 0019 cost = 0.000609\n",
      "Epoch:: 0020 cost = 0.000504\n",
      "Epoch:: 0021 cost = 0.000423\n",
      "Epoch:: 0022 cost = 0.000362\n",
      "Epoch:: 0023 cost = 0.000313\n",
      "Epoch:: 0024 cost = 0.000275\n",
      "Epoch:: 0025 cost = 0.000244\n",
      "Epoch:: 0026 cost = 0.000219\n",
      "Epoch:: 0027 cost = 0.000199\n",
      "Epoch:: 0028 cost = 0.000182\n",
      "Epoch:: 0029 cost = 0.000167\n",
      "Epoch:: 0030 cost = 0.000155\n",
      "Epoch:: 0031 cost = 0.000145\n",
      "Epoch:: 0032 cost = 0.000136\n",
      "Epoch:: 0033 cost = 0.000128\n",
      "Epoch:: 0034 cost = 0.000121\n",
      "Epoch:: 0035 cost = 0.000115\n",
      "Epoch:: 0036 cost = 0.000109\n",
      "Epoch:: 0037 cost = 0.000104\n",
      "Epoch:: 0038 cost = 0.000100\n",
      "Epoch:: 0039 cost = 0.000096\n",
      "Epoch:: 0040 cost = 0.000092\n",
      "Epoch:: 0041 cost = 0.000089\n",
      "Epoch:: 0042 cost = 0.000085\n",
      "Epoch:: 0043 cost = 0.000083\n",
      "Epoch:: 0044 cost = 0.000080\n",
      "Epoch:: 0045 cost = 0.000078\n",
      "Epoch:: 0046 cost = 0.000076\n",
      "Epoch:: 0047 cost = 0.000074\n",
      "Epoch:: 0048 cost = 0.000072\n",
      "Epoch:: 0049 cost = 0.000070\n",
      "Epoch:: 0050 cost = 0.000068\n",
      "Epoch:: 0051 cost = 0.000067\n",
      "Epoch:: 0052 cost = 0.000065\n",
      "Epoch:: 0053 cost = 0.000064\n",
      "Epoch:: 0054 cost = 0.000063\n",
      "Epoch:: 0055 cost = 0.000062\n",
      "Epoch:: 0056 cost = 0.000061\n",
      "Epoch:: 0057 cost = 0.000060\n",
      "Epoch:: 0058 cost = 0.000059\n",
      "Epoch:: 0059 cost = 0.000058\n",
      "Epoch:: 0060 cost = 0.000057\n",
      "Epoch:: 0061 cost = 0.000056\n",
      "Epoch:: 0062 cost = 0.000056\n",
      "Epoch:: 0063 cost = 0.000055\n",
      "Epoch:: 0064 cost = 0.000054\n",
      "Epoch:: 0065 cost = 0.000053\n",
      "Epoch:: 0066 cost = 0.000053\n",
      "Epoch:: 0067 cost = 0.000052\n",
      "Epoch:: 0068 cost = 0.000052\n",
      "Epoch:: 0069 cost = 0.000051\n",
      "Epoch:: 0070 cost = 0.000051\n",
      "Epoch:: 0071 cost = 0.000050\n",
      "Epoch:: 0072 cost = 0.000050\n",
      "Epoch:: 0073 cost = 0.000049\n",
      "Epoch:: 0074 cost = 0.000049\n",
      "Epoch:: 0075 cost = 0.000048\n",
      "Epoch:: 0076 cost = 0.000048\n",
      "Epoch:: 0077 cost = 0.000048\n",
      "Epoch:: 0078 cost = 0.000047\n",
      "Epoch:: 0079 cost = 0.000047\n",
      "Epoch:: 0080 cost = 0.000046\n",
      "Epoch:: 0081 cost = 0.000046\n",
      "Epoch:: 0082 cost = 0.000046\n",
      "Epoch:: 0083 cost = 0.000045\n",
      "Epoch:: 0084 cost = 0.000045\n",
      "Epoch:: 0085 cost = 0.000045\n",
      "Epoch:: 0086 cost = 0.000044\n",
      "Epoch:: 0087 cost = 0.000044\n",
      "Epoch:: 0088 cost = 0.000044\n",
      "Epoch:: 0089 cost = 0.000043\n",
      "Epoch:: 0090 cost = 0.000043\n",
      "Epoch:: 0091 cost = 0.000043\n",
      "Epoch:: 0092 cost = 0.000042\n",
      "Epoch:: 0093 cost = 0.000042\n",
      "Epoch:: 0094 cost = 0.000042\n",
      "Epoch:: 0095 cost = 0.000042\n",
      "Epoch:: 0096 cost = 0.000041\n",
      "Epoch:: 0097 cost = 0.000041\n",
      "Epoch:: 0098 cost = 0.000041\n",
      "Epoch:: 0099 cost = 0.000041\n",
      "Epoch:: 0100 cost = 0.000040\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# 신경망 학습\n",
    "##############\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                      feed_dict = {enc_input : input_batch,\n",
    "                                  dec_input : output_batch,\n",
    "                                  targets : target_batch})\n",
    "    print('Epoch::','%04d' % (epoch + 1),\n",
    "         'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "print('최적화 완료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# 결과 예측\n",
    "#############\n",
    "\n",
    "# 단어를 입력받아 번역 단어를 예측\n",
    "def translate(word):\n",
    "    # 예측시에는 한글 데이터를 모름. 따라서 디코더 입출력을 의미없는 P로 채운다.\n",
    "    seq_data = [word, 'P' * len(word)]\n",
    "    \n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    \n",
    "    \n",
    "    # 예측 모델을 돌린다.\n",
    "    \n",
    "    # 세번째 차원을 argmax로 취해 가장 확률이 높은 글자(인덱스)를 예측값으로 만든다.\n",
    "    prediction = tf.argmax(model, 2)\n",
    "\n",
    "    result = sess.run(prediction,\n",
    "                     feed_dict = {enc_input: input_batch,\n",
    "                                 dec_input: output_batch,\n",
    "                                 targets: target_batch})\n",
    "    # 결과는 인덱스 -> 각 인덱스에 맞는 숫자를 가져와 배열을 만든다.\n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    # 출력 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "예를 들어 \n",
    "1. 입력으로 'word'를 받으면,  \n",
    "2. seq_data는 ['word','PPPP']로 구성되고\n",
    "3. input_batch는 ['w','o','r','d'],  output_batch는 ['P','P','P','P'] 글자들을 원-핫 인코딩한 값일 것.\n",
    "4. target_batch는 ['P','P','P','P']의 인덱스인 [2,2,2,2]일 것이다.\n",
    "5. argmax로 예측값을 만든다.\n",
    "6. 인덱스에 해당하는 글자를 가져와 배열로 만든다.\n",
    "7. 출력 끝을 의미하는 'E' 이후 글자를 지우고 문자열로 만들어 반환\n",
    "\n",
    "최종 반환 결과는 ['사','랑','E','E'] 형식으로 나온다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ======= 번역 테스트======\n",
      "word -> 단어\n",
      "wodr -> 나무\n",
      "love -> 사랑\n",
      "loev -> 사랑\n",
      "abcd -> 단어키무\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# 번역 시도\n",
    "############\n",
    "print(\"\\n ======= 번역 테스트======\")\n",
    "print('word ->',translate('word'))\n",
    "print('wodr ->',translate('wodr'))\n",
    "print('love ->',translate('love'))\n",
    "print('loev ->',translate('loev'))\n",
    "print('abcd ->',translate('abcd'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 10.4 더보기\n",
    "Seq2Seq 모델로 만들 수 있는 유용한 응용분야 중 문서 요약이 있다.  \n",
    "    - Text Summarization\n",
    "자연어 분석에 사용\n",
    "    - Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
