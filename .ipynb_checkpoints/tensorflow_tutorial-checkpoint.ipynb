{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TensorFlow 기본\n",
    "\n",
    "#### 3.1 텐서와 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n",
      "Tensor(\"Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.constant : 텐서플로 상수.\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)  # a + b로도 쓸 수 있다.\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "hello가 텐서플로의 **텐서**라는 자료형이고, 상수를 담는다.\n",
    "\n",
    "텐서는 텐서플로에서 다양한 수학식을 계산하기 위한 가장 기본적이고 중요한 자료형이며 **랭크**와 **셰이프**라는 개념을 가지고있다.\n",
    "\n",
    "* 3 : 랭크가 0인 텐서; 셰이프는 []\n",
    "* [1. ,2. ,3.] : 랭크가 1인 텐서; 셰이프는 [3]\n",
    "* [[1., 2., 3.], [4., 5., 6.]] : 랭크가 2인 텐서; 셰이프는 [2, 3]\n",
    "* [[[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]]] : 랭크가 3인 텐서; 셰이프는 [2, 1, 3]\n",
    "\n",
    "텐서 자료형의 형태는 배열과 비슷하다. **랭크**는 차원의 수이다. \n",
    "* 랭크 0 : 스칼라\n",
    "* 랭크 1 : 벡터\n",
    "* 랭크 2 : 행렬\n",
    "* 랭크3 : n-TensorFlow, n차원 텐서\n",
    "**셰이프**는 각 차원의 요소 개수\n",
    "\n",
    "텐서를 출력할 때 나오는 **dtype은 해당 텐서에 담긴 요소들의 자료형**이다. string, float, int등이 올 수 있다.\n",
    "\n",
    "\n",
    "위에서 변수와 수식들을 정의해씨만, 실행이 정의한 시점에서 실행되는 것은 아니다.\n",
    "\n",
    "그 이유는 텐서플로 프로그램 구조가 \n",
    "1. 그래프 생성\n",
    "2. 그래프 실행\n",
    "\n",
    "두 가지로 분리되어 있기 때문이다.\n",
    "\n",
    "![텐서플로 구조](./img/img1.JPG)\n",
    "\n",
    "**그래프**는 텐서들의 연산 모음이다. \n",
    "\n",
    "텐서플로는 (1)텐서와 텐서의 연산들을 먼저 정의하여 그래프를 만들고\n",
    "(2) 이후 필요할 때 연산을 실행하는 코드를 넣어 *'원하는 시점'*에 실제 연산을 수행하도록 한다. \n",
    "이러한 방식은 **지연 실행**이라하며 함수형 프로그래밍에서 많이 사용한다.\n",
    "\n",
    "다음처럼 Session 객체와 run 메소드를 사용할 때 계산이 된다.\n",
    "\n",
    "따라서 모델을 구성하는 것과, 실행하는 것을 분리하여 프로그램을 깔끔하게 작성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "# 그래프를 실행할 세션을 구성한다.\n",
    "sess = tf.Session()\n",
    "\n",
    "# sess.run : 설정한 텐서 그래프(변수나 수식)을 실행한다.\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "# 세션을 닫는다\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프의 실행은 Session안에서 이루어져야 하며, session객체와 run 메소드를 사용한다.\n",
    "\n",
    "\n",
    "#### 3.2 플레이스홀더와 변수\n",
    "* 플레이스폴더\n",
    "\n",
    "   그래프에 사용할 입력값을 나중에 받기 위해 사용하는 매개변수\n",
    "   \n",
    "   !. 변수는 그래프를 최적화하는 용도로 텐서플로가(학습함수들이) 학습한 결과를 갱신하기 위해 사용하는 변수이다.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(?, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# None은 크기가 정해지지 않았음을 의미\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "float32 자료형을 가진 텐서가 생성됐다.\n",
    "\n",
    "나중에 플레이스홀더 X에 넣을 자료를 다음과 같이 정의해 볼 수 있다.\n",
    "\n",
    "앞서 텐서 모양을 (?, 3)으로 정의했으므로, 두 번째 차원은 요소를 3개씩 가지고 있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2,3], [4,5,6]]\n",
    "\n",
    "#변수 정의\n",
    "W = tf.Variable(tf.random_normal([3, 2]))  # random_normal : 정규분포의 무작위 값\n",
    "b = tf.Variable(tf.random_normal([2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "각각 W와 b에 텐서플로 변수를 생성하여 할당한다.\n",
    "\n",
    "W는 [3, 2] 행렬형태의 텐서, b는 [2, 1] 행렬 형대의 텐서로 정규분포의 무작위 값으로 초기화했다.\n",
    "\n",
    "물론 다른 함수를 쓰거나 특정 형태의 데이터를 만들어서 넣어줄 수도 있다.\n",
    "\n",
    "\n",
    "다음으로 입력값과 변수들을 계산할 수식을 작성한다.\n",
    "\n",
    "X와 W가 행렬이기 때문에 marmul 함수를 사용해햐 한다.\n",
    "\n",
    "*행렬이 아닌 경우에는 단순히 곱셈 연산자나 mul함수를 사용*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**행렬곱 매우 중요**\n",
    "\n",
    "![행렬곱](./img/img2.JPG)\n",
    "\n",
    "위 정의에 따라 앞서 X에 넣을 데이터를 [2,3] 형태로 정의하였으므로 행렬곱을 위해 W의 형태를 [3,2]로 정의한 것.\n",
    "\n",
    "이제 연산을 실행하고 결과를 출력하여, 설정한 텐서들과 계산됭 그래프의 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[-0.88866204 -0.09361325]\n",
      " [ 0.35012633  0.88960224]\n",
      " [ 0.550321   -0.6291377 ]]\n",
      "=== b ===\n",
      "[[ 2.053232]\n",
      " [-1.285216]]\n",
      "=== expr ===\n",
      "[[ 3.5157855   1.8514102 ]\n",
      " [ 0.21269333 -0.9864837 ]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())   # 앞서 정의한 변수들을 초기화하는 함수.\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "print(sess.run(expr, feed_dict={X: x_data}))    # feed_dict : 그래프를 실행할 때 사용할 입력값을 지정한다.\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "global_variables_initializer : 기존에 학습한 값을 가져와서 사용하는 것이 아닌 처음 실행하는 것이라면 연산을 실행하기 전에\n",
    "    반드시 이 함수를 이용해 변수들을 초기화해야 한다.\n",
    "    \n",
    "feed_dict : expr 수식에는 X, W, b를 사용했는데, 이 중 X가 플레이스홀더라 X에 값을 넣어주지 않으면 계산에 사용할 수 없기 때문에\n",
    "    에러가 난다. 따라서 미리 정의해둔 x_data를 X의 값으로 넣어준다. ***딕셔너리여도 되나??***\n",
    "    \n",
    "\n",
    "#### 3.3 선형 회귀 모델 구현\n",
    "**선형 회귀**란 주어진 x와 y값을 가지고 서로 간의 관계를 파악하는 것\n",
    "\n",
    "이 관계를 알고나면 새로운 x가 주어졌을 때 y의 값을 쉽게 알 수 있다. \n",
    "\n",
    "![선형 회귀 그래프](./img/img3.JPG)\n",
    "\n",
    "\n",
    "이제 텐서플로의 최적화 함수를 이용해 X와 Y의 상관관계를 분석하는 기초적인 선형 회귀 모델을 만들고 실행해본다.\n",
    "\n",
    "다음과 같이 주어진 x_data와 y_data의 상관관계를 파악해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [1, 2, 3]\n",
    "\n",
    "\n",
    "# x와 y의 상관관계를 설명하기 위한 변수들인 W, b를 \n",
    "# 각각 -0.1 ~ 1.0 사이의 균등분포를 가진 무작위값으로 초기화한다.\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "# 자료를 입력받을 플레이스홀더 설정\n",
    "# name: 나중에 텐서보드등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙여줍니다.\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "# X와 Y의 상관관계(여기서는 선형관계)를 분석하기 위한 수실을 작성한다.\n",
    "hypothesis = W * X + b    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 수식은 W와의 곱과 b와의 합을 통해 X와 Y의 관계를 설명하겠다는 뜻이다.\n",
    "\n",
    "**다시말해서 X가 주어졌을 때 Y를 만들어 낼 수 있는 W와 b를 찾아내겠다는 의미.**\n",
    "\n",
    "W는 **가중치**, b는 **편향**이라고 하고, 위 수식은 신경망 학습에서 가장 기본이 되는 수식이다.\n",
    "여기서는 w와 X가 행렬이 아니므로 tf.matmul 함수가 아니라 곱셈 연산자(*)를 사용했다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수를 작성한다.\n",
    "# mean(h - Y)^2 : 예측값과 실제값의 거리를 비용(손실) 함수로 정합니다.\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**손실 함수(loss function)**는 한 쌍(x,y)의 데이터에 대한 손실값을 계산하는 함수이다.\n",
    "\n",
    "손실값이란 실제값과 모델로 예측한 가설의 값이 얼마나 차이가 나는가를 나타내는 값이다.\n",
    "\n",
    "즉, 손실값이 작을수록 그 모델이 X와 Y의 관계를 잘 설명하고 있는다는 뜻으로 *X값에 대한 Y값을 정확하게 예측할 수 있다*는 뜻이 된다.\n",
    "\n",
    "이 손실을 전체 데이터에 대해 구한 경우 이를 **비용**이라고 한다.\n",
    "\n",
    "즉, **학습**이란 변수들의 값을 다양하게 넣어 계산해보면서 이 **손실값을 최소화하는 W와 b의 값을 구하는 것**이다.\n",
    "\n",
    "\n",
    "손실값으로는 '예측값과 실제값의 거리'를 가장 많이 사용한다. \n",
    "\n",
    "손실값은 예측값에서 실제값을 뺀 뒤 제곱하는데, 비용은 모든 데이터에 대한 손실값의 평균을 내어 구한다.\n",
    "\n",
    "  \n",
    "마지막으로 경사하강법 최적화 함수를 이용해 손실값을 최소화하는 연산 그래프를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 경사하강법 최적화 함수를 이용해 손실값을 최소화하는 연산 그래프 생성\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최적화함수**란 가중치와 편향의 값을 변경해가면서 손실값을 최소화하는 *가장 최적화된 가중치와 편향 값을 찾아주는 함수*이다.\n",
    "\n",
    "이때 값들을 무작위로 변경하면 시간이 오래걸리고 예측도 쉽지 않다. 따라서 빠르게 최적화하기 위한 다양한 방법을 사용한다.\n",
    "\n",
    "경사하강법은 최적화 방법 중 가장 기본적인 알고리즘으로,\n",
    "\n",
    "![경사하강법](./img/img4.JPG)  \n",
    "함수의 기울기를 구하고 기울기가 낮은 족으로 계속 이동시키면서 최적의 값을 찾아가는 방법이다.\n",
    "\n",
    "매개변수인 learning_rate는 **학습률**로 학습을 얼마나 \"급하게\"할 것인가를 설정하는 값이다. 값이 너무 크거나 작으면 좋지 않다.\n",
    "(크면 최적의 값을 찾지 못하고, 작으면 속도가 느리다)\n",
    "\n",
    "이렇게 학습 진행 과정에 영향을 주는 변수를 **하이퍼파라미터**라고 하며, 이 값에 따라 속도나 성능이 크게 달라질 수 있다.\n",
    "\n",
    "이제 모델은 다 만들었고, 그래프를 실행해 학습시키고 결과를 확인한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.27773085 [0.5816037] [0.9928714]\n",
      "1 0.14106424 [0.5749584] [0.9616557]\n",
      "2 0.13288867 [0.5870016] [0.9393412]\n",
      "3 0.12655877 [0.5967303] [0.9166723]\n",
      "4 0.1205469 [0.60644644] [0.8946457]\n",
      "5 0.11482086 [0.6159048] [0.87313795]\n",
      "6 0.10936674 [0.62513846] [0.8521484]\n",
      "7 0.1041717 [0.63414985] [0.83166337]\n",
      "8 0.09922349 [0.64294463] [0.8116707]\n",
      "9 0.09451029 [0.651528] [0.7921587]\n",
      "10 0.090021014 [0.659905] [0.77311575]\n",
      "11 0.08574492 [0.6680807] [0.7545306]\n",
      "12 0.08167198 [0.6760598] [0.7363922]\n",
      "13 0.07779249 [0.68384707] [0.71868986]\n",
      "14 0.07409727 [0.69144714] [0.70141304]\n",
      "15 0.07057763 [0.6988646] [0.6845516]\n",
      "16 0.067225106 [0.7061036] [0.6680954]\n",
      "17 0.06403191 [0.71316874] [0.6520349]\n",
      "18 0.060990345 [0.7200639] [0.6363604]\n",
      "19 0.058093254 [0.7267934] [0.62106276]\n",
      "20 0.055333775 [0.7333611] [0.6061328]\n",
      "21 0.05270542 [0.739771] [0.5915618]\n",
      "22 0.050201822 [0.74602664] [0.577341]\n",
      "23 0.047817186 [0.752132] [0.56346214]\n",
      "24 0.04554588 [0.7580906] [0.5499169]\n",
      "25 0.043382373 [0.7639059] [0.53669727]\n",
      "26 0.0413217 [0.7695815] [0.5237955]\n",
      "27 0.039358873 [0.77512056] [0.51120377]\n",
      "28 0.03748929 [0.7805265] [0.4989148]\n",
      "29 0.035708573 [0.78580254] [0.48692125]\n",
      "30 0.034012362 [0.79095167] [0.47521597]\n",
      "31 0.032396767 [0.79597706] [0.46379212]\n",
      "32 0.0308579 [0.8008816] [0.45264286]\n",
      "33 0.029392116 [0.8056683] [0.4417616]\n",
      "34 0.027995946 [0.8103399] [0.43114197]\n",
      "35 0.026666136 [0.81489927] [0.42077762]\n",
      "36 0.02539947 [0.8193489] [0.41066238]\n",
      "37 0.024192961 [0.8236916] [0.40079036]\n",
      "38 0.023043783 [0.8279299] [0.39115563]\n",
      "39 0.021949196 [0.8320664] [0.38175255]\n",
      "40 0.020906614 [0.83610344] [0.3725755]\n",
      "41 0.019913519 [0.84004337] [0.363619]\n",
      "42 0.018967599 [0.8438886] [0.35487783]\n",
      "43 0.018066624 [0.8476414] [0.34634683]\n",
      "44 0.01720845 [0.85130405] [0.33802092]\n",
      "45 0.016391046 [0.8548786] [0.32989514]\n",
      "46 0.01561246 [0.8583672] [0.32196468]\n",
      "47 0.014870856 [0.86177194] [0.31422484]\n",
      "48 0.0141644925 [0.8650949] [0.3066711]\n",
      "49 0.013491653 [0.86833787] [0.2992989]\n",
      "50 0.01285078 [0.87150294] [0.29210398]\n",
      "51 0.012240354 [0.87459195] [0.285082]\n",
      "52 0.011658926 [0.87760663] [0.27822882]\n",
      "53 0.011105127 [0.88054895] [0.2715404]\n",
      "54 0.010577627 [0.8834204] [0.26501274]\n",
      "55 0.010075196 [0.8862229] [0.25864202]\n",
      "56 0.009596606 [0.88895804] [0.25242445]\n",
      "57 0.009140749 [0.89162743] [0.24635635]\n",
      "58 0.008706565 [0.89423263] [0.24043411]\n",
      "59 0.008292996 [0.8967752] [0.23465423]\n",
      "60 0.007899082 [0.89925665] [0.22901331]\n",
      "61 0.007523867 [0.9016785] [0.223508]\n",
      "62 0.0071664653 [0.904042] [0.218135]\n",
      "63 0.006826054 [0.90634876] [0.21289119]\n",
      "64 0.006501818 [0.9086001] [0.20777345]\n",
      "65 0.006192978 [0.9107973] [0.20277873]\n",
      "66 0.005898807 [0.91294163] [0.19790407]\n",
      "67 0.005618608 [0.9150345] [0.1931466]\n",
      "68 0.0053517236 [0.917077] [0.18850349]\n",
      "69 0.005097508 [0.9190704] [0.183972]\n",
      "70 0.004855378 [0.9210159] [0.17954944]\n",
      "71 0.004624739 [0.9229146] [0.1752332]\n",
      "72 0.004405064 [0.9247677] [0.17102072]\n",
      "73 0.004195818 [0.92657626] [0.16690952]\n",
      "74 0.003996516 [0.92834127] [0.16289711]\n",
      "75 0.0038066825 [0.9300639] [0.15898119]\n",
      "76 0.0036258616 [0.9317452] [0.1551594]\n",
      "77 0.003453619 [0.93338585] [0.15142943]\n",
      "78 0.0032895773 [0.9349873] [0.14778921]\n",
      "79 0.0031333212 [0.93655014] [0.14423645]\n",
      "80 0.0029844828 [0.93807536] [0.14076908]\n",
      "81 0.0028427206 [0.93956405] [0.13738512]\n",
      "82 0.0027076832 [0.9410169] [0.13408248]\n",
      "83 0.002579068 [0.9424348] [0.13085921]\n",
      "84 0.0024565673 [0.9438186] [0.12771346]\n",
      "85 0.0023398772 [0.94516915] [0.1246433]\n",
      "86 0.002228728 [0.9464873] [0.12164699]\n",
      "87 0.002122869 [0.94777375] [0.11872269]\n",
      "88 0.0020220275 [0.94902915] [0.11586864]\n",
      "89 0.0019259813 [0.9502545] [0.11308326]\n",
      "90 0.0018344905 [0.9514503] [0.11036479]\n",
      "91 0.0017473547 [0.95261747] [0.10771173]\n",
      "92 0.0016643492 [0.95375645] [0.10512239]\n",
      "93 0.0015852948 [0.95486814] [0.10259533]\n",
      "94 0.0015099873 [0.95595306] [0.10012902]\n",
      "95 0.0014382672 [0.95701194] [0.09772198]\n",
      "96 0.0013699448 [0.9580453] [0.0953728]\n",
      "97 0.0013048725 [0.9590539] [0.0930801]\n",
      "98 0.0012428967 [0.96003824] [0.09084255]\n",
      "99 0.0011838536 [0.96099883] [0.08865874]\n",
      "\n",
      "=====Test=========\n",
      "X: 5, Y: [4.893653]\n",
      "X: 2.5, Y: [2.4911559]\n"
     ]
    }
   ],
   "source": [
    "# 세션은 생성하고 변수들을 초기화 한다. \n",
    "# 파이썬 with 기능을 이용해 세션 블록을 만들고 종료를 자동으로 처리하도록 했다.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # 최적화를 수행하는 그래프인 train_op를 실행하고\n",
    "    # 실행 시마다 변화하는 손실값을 출력하는 코드\n",
    "    # 학습은 100번 수행하며 feed_dict 매개변수로 상관관계를 알아내고자 하는 x_data, y_data를 입력해준다.\n",
    "    for step in range(100):\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "        print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 나오는지 확인\n",
    "    print(\"\\n=====Test=========\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실값과 변수들의 변화를 확인 할 수 있다.\n",
    "\n",
    "손실값이 점점 줄어든다면 학습이 정상적으로 이뤄지고있는 것이다.\n",
    "\n",
    "*** _ 는 뭐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
